
# -*- coding: utf-8 -*-
"""
utils.py ‚Äî Helpers de EDA (Exploratory Data Analysis)
=====================================================

Este m√≥dulo proporciona herramientas completas para an√°lisis exploratorio de datos:

FUNCIONALIDADES PRINCIPALES:
- Separaci√≥n autom√°tica de tipos de columnas (num√©ricas, categ√≥ricas, fechas, etc.)
- An√°lisis univariado (variables categ√≥ricas y num√©ricas)
- An√°lisis bivariado (relaci√≥n feature ‚Üî target)
- An√°lisis de correlaciones, asociaciones, VIF y Mutual Information

CONVENCIONES:
- Todas las funciones de plotting devuelven (fig, axes) y aceptan show=True/False
- NO se usan ejes gemelos (twinx) para evitar confusi√≥n
- Histogramas y Boxplots van en ejes separados para mayor claridad
"""

# =============================================================================
# IMPORTS Y CONFIGURACI√ìN
# =============================================================================

from __future__ import annotations  # Permite usar tipos como strings
from typing import Iterable, List, Tuple, Optional, Dict

# Librer√≠as principales para an√°lisis de datos
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Librer√≠as opcionales (con manejo de errores)
try:
    from statsmodels.stats.outliers_influence import variance_inflation_factor as _vif
    _HAS_STATSMODELS = True
except Exception:
    _HAS_STATSMODELS = False  # Para calcular VIF (Variance Inflation Factor)

try:
    from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
    _HAS_SKLEARN = True
except Exception:
    _HAS_SKLEARN = False  # Para calcular Mutual Information

# Funciones de pandas para detectar tipos de datos
from pandas.api.types import (
    is_numeric_dtype, is_bool_dtype, is_datetime64_any_dtype, is_categorical_dtype
)

# =============================================================================
# FUNCIONES UTILITARIAS
# =============================================================================

def _ensure_list(x):
    """
    Convierte cualquier entrada en una lista.
    
    Args:
        x: Puede ser None, lista, tupla, array de numpy, o √≠ndice de pandas
        
    Returns:
        list: Lista con los elementos de x
    """
    if x is None: return []
    if isinstance(x, (list, tuple, np.ndarray, pd.Index)): return list(x)
    return [x]

# =============================================================================
# DETECCI√ìN AUTOM√ÅTICA DE TIPOS DE COLUMNAS
# =============================================================================

def detectar_tipos_columnas(
    df: pd.DataFrame,
    target: Optional[str] = None,
    low_card_num_as_cat: int = 15,
    treat_bool_as_cat: bool = True,
    coerce_numeric_objects: bool = True,
    infer_datetimes: bool = True,
    id_name_patterns: Tuple[str, ...] = ("id","uuid","guid","code","cod","dni","nif","zip","postal"),
) -> Dict[str, List[str]]:
    """
    Detecta autom√°ticamente los tipos de columnas en un DataFrame.
    
    Esta funci√≥n es fundamental para el EDA ya que clasifica las columnas en:
    - Num√©ricas: Para an√°lisis estad√≠stico y correlaciones
    - Categ√≥ricas: Para an√°lisis de frecuencias y asociaciones
    - Fechas: Para an√°lisis temporal
    - IDs: Columnas que no aportan informaci√≥n (se excluyen del an√°lisis)
    
    Args:
        df: DataFrame a analizar
        target: Nombre de la columna objetivo (se excluye de features)
        low_card_num_as_cat: N√∫mero m√°ximo de valores √∫nicos para considerar 
                            una variable num√©rica como categ√≥rica
        treat_bool_as_cat: Si True, trata las variables booleanas como categ√≥ricas
        coerce_numeric_objects: Si True, intenta convertir objetos a num√©ricos
        infer_datetimes: Si True, intenta detectar columnas de fechas
        id_name_patterns: Patrones en nombres de columnas que indican IDs
        
    Returns:
        Dict con listas de columnas clasificadas por tipo
    """
    dfw = df.copy()  # Trabajamos en una copia para no modificar el original

    # PASO 1: Intentar convertir objetos a num√©ricos
    if coerce_numeric_objects:
        for c in dfw.select_dtypes(include=["object"]).columns:
            # Reemplazar comas por puntos (formato europeo)
            s = dfw[c].astype(str).str.replace(",", ".", regex=False)
            conv = pd.to_numeric(s, errors="coerce")
            # Si al menos 95% se convirti√≥ exitosamente, usar el resultado
            if conv.notna().mean() >= 0.95:
                dfw[c] = conv

    # PASO 2: Intentar detectar columnas de fechas
    if infer_datetimes:
        for c in dfw.select_dtypes(include=["object"]).columns:
            try:
                # Intentar convertir a datetime (formato europeo: d√≠a/mes/a√±o)
                conv = pd.to_datetime(dfw[c], errors="coerce", utc=False, dayfirst=True)
                # Si al menos 90% se convirti√≥ exitosamente, usar el resultado
                if conv.notna().mean() >= 0.90:
                    dfw[c] = conv
            except Exception:
                pass  # Si falla, continuar con la siguiente columna

    # PASO 3: Clasificar columnas por tipo b√°sico
    numeric = [c for c in dfw.columns if is_numeric_dtype(dfw[c])]      # Variables num√©ricas
    boolean = [c for c in dfw.columns if is_bool_dtype(dfw[c])]         # Variables booleanas
    dates   = [c for c in dfw.columns if is_datetime64_any_dtype(dfw[c])]  # Variables de fecha
    cats_raw = dfw.select_dtypes(include=["object"]).columns.tolist()    # Objetos (strings)
    cats_cat = [c for c in dfw.columns if is_categorical_dtype(dfw[c])] # Categ√≥ricas expl√≠citas
    categorical = sorted(set(cats_raw + cats_cat))  # Unir todas las categ√≥ricas

    # PASO 4: Detectar columnas tipo ID (no √∫tiles para an√°lisis)
    n = len(dfw)
    id_like = []
    for c in dfw.columns:
        if c == target: continue  # No excluir el target
        
        # Verificar si el nombre contiene patrones de ID
        name_hit = any(p in c.lower() for p in id_name_patterns)
        
        # Verificar si tiene cardinalidad muy alta (casi √∫nica)
        try:
            high_card = dfw[c].nunique(dropna=True) >= 0.98 * n
        except Exception:
            high_card = False
            
        # Si cumple cualquiera de los criterios, es un ID
        if name_hit or high_card:
            id_like.append(c)

    # PASO 5: Refinar clasificaci√≥n para an√°lisis
    # Variables num√©ricas con baja cardinalidad ‚Üí tratarlas como categ√≥ricas
    num_low_card = [c for c in numeric if dfw[c].nunique(dropna=True) <= low_card_num_as_cat]
    numeric_for_analysis = [c for c in numeric if c not in num_low_card]  # Resto de num√©ricas
    
    # Unir todas las categ√≥ricas (incluyendo booleanas y num√©ricas de baja cardinalidad)
    categorical_for_analysis = sorted(set(categorical + (boolean if treat_bool_as_cat else []) + num_low_card))

    # PASO 6: Limpiar listas excluyendo columnas no √∫tiles para an√°lisis
    def _clean(nums, cats):
        """Excluye target, fechas e IDs de las listas de features"""
        nums2 = [c for c in nums if c != target and c not in dates and c not in id_like]
        cats2 = [c for c in cats if c != target and c not in dates and c not in id_like]
        return nums2, cats2

    numeric_features, categorical_features = _clean(numeric_for_analysis, categorical_for_analysis)

    # PASO 7: Preparar resultado final
    out = {
        "numeric_all": numeric,                    # Todas las num√©ricas (incluyendo IDs)
        "categorical_all": categorical,            # Todas las categ√≥ricas
        "boolean": boolean,                        # Variables booleanas
        "dates": dates,                           # Variables de fecha
        "id_like": id_like,                       # Columnas tipo ID
        "numeric_for_analysis": numeric_for_analysis,      # Num√©ricas para an√°lisis
        "categorical_for_analysis": categorical_for_analysis,  # Categ√≥ricas para an√°lisis
        "numeric_features": numeric_features,      # Num√©ricas limpias (sin target/IDs/fechas)
        "categorical_features": categorical_features,  # Categ√≥ricas limpias
    }
    out["features_all"] = out["numeric_features"] + out["categorical_features"]  # Todas las features
    return out

# =============================================================================
# REPORTES R√ÅPIDOS DE CALIDAD DE DATOS
# =============================================================================

def resumen_nulos(df: pd.DataFrame) -> pd.DataFrame:
    """
    Genera un resumen de valores nulos por columna.
    
    Args:
        df: DataFrame a analizar
        
    Returns:
        DataFrame con columnas 'n_nulos' y 'pct_nulos', ordenado por % de nulos descendente
    """
    n = len(df)
    return (
        df.isna().sum().to_frame("n_nulos")
          .assign(pct_nulos=lambda x: x["n_nulos"]/n)
          .sort_values("pct_nulos", ascending=False)
    )

def resumen_numericas(df: pd.DataFrame, cols: Optional[Iterable[str]] = None,
                      percentiles=(0.01,0.05,0.25,0.5,0.75,0.95,0.99)) -> pd.DataFrame:
    """
    Genera estad√≠sticas descriptivas para variables num√©ricas.
    
    Args:
        df: DataFrame a analizar
        cols: Columnas espec√≠ficas a analizar (si None, usa todas las num√©ricas)
        percentiles: Lista de percentiles a calcular
        
    Returns:
        DataFrame con estad√≠sticas descriptivas por variable
    """
    cols = _ensure_list(cols) or df.select_dtypes(include="number").columns.tolist()
    percentiles = list(sorted(set(percentiles)))
    return df[cols].describe(percentiles=percentiles).T

def reporte_categorias_raras(df: pd.DataFrame, cat_cols: Iterable[str], threshold: float=0.01) -> pd.DataFrame:
    """
    Identifica categor√≠as con frecuencia muy baja (posibles errores o outliers).
    
    Args:
        df: DataFrame a analizar
        cat_cols: Columnas categ√≥ricas a revisar
        threshold: Umbral m√≠nimo de frecuencia (por defecto 1%)
        
    Returns:
        DataFrame con categor√≠as que est√°n por debajo del umbral
    """
    rows = []
    for c in _ensure_list(cat_cols):
        # Calcular frecuencias relativas (incluyendo nulos)
        vc = df[c].value_counts(dropna=False, normalize=True)
        # Identificar categor√≠as con frecuencia menor al umbral
        raras = vc[vc < threshold]
        for k, v in raras.items():
            rows.append({"columna": c, "categoria": k, "freq": v})
    return pd.DataFrame(rows).sort_values(["columna","freq"])

# =============================================================================
# AN√ÅLISIS UNIVARIADO - VARIABLES CATEG√ìRICAS
# =============================================================================

def graficar_categoricas(df, cat_var, figsize=(15, 10), max_categories=20, show_percentages=True):
    """
    Funci√≥n para graficar m√∫ltiples variables categ√≥ricas usando un bucle for
    
    Par√°metros:
    - df: DataFrame con los datos
    - variables_categoricas: lista de nombres de columnas categ√≥ricas
    - figsize: tama√±o de la figura completa
    - max_categories: n√∫mero m√°ximo de categor√≠as a mostrar por variable
    - show_percentages: si mostrar porcentajes adem√°s de valores absolutos
    """
    
    # Calcular n√∫mero de filas y columnas para los subplots
    n_vars = len(cat_var)
    n_cols = 2  # 3 gr√°ficos por fila
    n_rows = (n_vars + n_cols - 1) // n_cols  # Redondear hacia arriba
    
    # Crear la figura con subplots
    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    
    # Asegurar que axes sea siempre un array 2D
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    elif n_cols == 1:
        axes = axes.reshape(-1, 1)
    
    # Definir una paleta de colores
    colores = plt.cm.Set3(range(12))
    
    # Bucle for para crear cada gr√°fico
    for i, variable in enumerate(cat_var):
        row = i // n_cols
        col = i % n_cols
        ax = axes[row, col]
        
        # Verificar si la variable existe en el DataFrame
        if variable not in df.columns:
            ax.text(0.5, 0.5, f'Variable "{variable}"\nno encontrada', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{variable} (No encontrada)')
            continue
        
        # Contar valores y obtener los m√°s frecuentes
        value_counts = df[variable].value_counts()
        
        # Limitar el n√∫mero de categor√≠as si es necesario
        if len(value_counts) > max_categories:
            value_counts = value_counts.head(max_categories)
            titulo = f'{variable} (Top {max_categories})'
        else:
            titulo = variable
        
        # Crear gr√°fico de barras
        bars = ax.bar(range(len(value_counts)), 
                     value_counts.values, 
                     color=colores[i % len(colores)],
                     alpha=0.7,
                     edgecolor='black',
                     linewidth=0.5)
        
        # Configurar el gr√°fico
        ax.set_title(titulo, fontsize=12, fontweight='bold', pad=10)
        ax.set_xlabel('Categor√≠as', fontsize=10)
        ax.set_ylabel('Frecuencia', fontsize=10)
        
        # Rotar etiquetas del eje x si son muchas o muy largas
        labels = [str(label) for label in value_counts.index]
        if len(labels) > 5 or any(len(str(label)) > 8 for label in labels):
            ax.set_xticks(range(len(value_counts)))
            ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)
        else:
            ax.set_xticks(range(len(value_counts)))
            ax.set_xticklabels(labels, fontsize=9)
        
        # A√±adir valores en las barras con rotaci√≥n y porcentajes
        total_count = value_counts.sum()
        
        # Determinar rotaci√≥n autom√°tica basada en el n√∫mero de categor√≠as
        if len(value_counts) > 8:
            rotation_angle = 45
        elif len(value_counts) > 5:
            rotation_angle = 30
        else:
            rotation_angle = 0
            
        for i, (bar, value) in enumerate(zip(bars, value_counts.values)):
            height = bar.get_height()
            percentage = (value / total_count) * 100
            
            # Texto con valor absoluto y porcentaje
            if show_percentages:
                text = f'{value:,}\n({percentage:.1f}%)'
            else:
                text = f'{value:,}'
            
            # Estrategia inteligente para posicionar texto seg√∫n el espacio disponible
            bar_width = bar.get_width()
            bar_height = bar.get_height()
            
            # Si hay muchas categor√≠as, usar estrategia m√°s conservadora
            if len(value_counts) > 10:
                # Solo mostrar texto en barras significativas (>5% del total)
                if percentage > 5:
                    # Posicionar texto dentro de la barra si es alta
                    if bar_height > ax.get_ylim()[1] * 0.1:
                        ax.text(bar.get_x() + bar_width/2., bar_height * 0.5,
                               text, ha='center', va='center', fontsize=8, 
                               rotation=rotation_angle, fontweight='bold')
                    else:
                        # Posicionar texto arriba de la barra
                        ax.text(bar.get_x() + bar_width/2., bar_height + bar_height*0.02,
                               text, ha='center', va='bottom', fontsize=8, 
                               rotation=rotation_angle)
                else:
                    # Para barras peque√±as, solo mostrar porcentaje
                    ax.text(bar.get_x() + bar_width/2., bar_height + bar_height*0.01,
                           f'{percentage:.1f}%', ha='center', va='bottom', fontsize=8, 
                           rotation=rotation_angle)
            else:
                # Para pocas categor√≠as, mostrar todo normalmente
                ax.text(bar.get_x() + bar_width/2., bar_height + bar_height*0.01,
                       text, ha='center', va='bottom', fontsize=8, rotation=rotation_angle)
        
        # Mejorar el aspecto del gr√°fico
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
    
    # Ocultar subplots vac√≠os si los hay
    for i in range(n_vars, n_rows * n_cols):
        row = i // n_cols
        col = i % n_cols
        axes[row, col].set_visible(False)
    
    # Ajustar el layout
    plt.tight_layout(pad=2.0)
    plt.show()




import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from scipy import stats
from collections import Counter

def detectar_tipo_numerico(data, variable_name):
    """
    Detecta autom√°ticamente el tipo de variable num√©rica.
    
    Esta funci√≥n clasifica las variables num√©ricas en:
    - ORDINAL: Variables con valores ordenados (ej: ratings 1-5, escalas)
    - DISCRETA: Variables con valores enteros espec√≠ficos (ej: n√∫mero de hijos)
    - CONTINUA: Variables con muchos valores √∫nicos (ej: precios, edades)
    
    Args:
        data: Serie de pandas con los datos
        variable_name: Nombre de la variable (para debugging)
        
    Returns:
        str: Tipo detectado ('ordinal', 'discreta', 'continua', 'sin_datos', 'no_numerica')
    """
    # Limpiar datos nulos
    clean_data = data.dropna()
    
    if len(clean_data) == 0:
        return 'sin_datos'
    
    # Convertir a num√©rico si es posible
    try:
        clean_data = pd.to_numeric(clean_data)
    except:
        return 'no_numerica'
    
    unique_values = clean_data.nunique()
    total_values = len(clean_data)
    
    # Verificar si son enteros
    es_entero = all(clean_data == clean_data.astype(int))
    
    # Criterios de clasificaci√≥n
    if unique_values <= 10 and es_entero:
        # Pocos valores √∫nicos y enteros -> Discreta/Ordinal
        if all(val >= 0 for val in clean_data.unique()) and max(clean_data) <= 10:
            return 'ordinal'  # Ej: ratings 1-5, escalas 1-10
        else:
            return 'discreta'  # Ej: n√∫mero de productos, hijos
    
    elif unique_values / total_values < 0.05 and es_entero:
        # Pocos valores √∫nicos relativos y enteros -> Discreta
        return 'discreta'
    
    elif es_entero and unique_values < 50:
        # Enteros con valores moderados -> Discreta
        return 'discreta'
    
    # Caso especial para variables float que representan ratings/escalas
    elif not es_entero and unique_values <= 20:
        # Verificar si es una escala de rating (0-5, 0-10, etc.)
        min_val = clean_data.min()
        max_val = clean_data.max()
        
        # Si est√° en un rango t√≠pico de ratings y tiene pocos valores √∫nicos
        if (min_val >= 0 and max_val <= 10 and unique_values <= 20):
            # Verificar si los valores est√°n espaciados de manera t√≠pica de ratings
            sorted_vals = sorted(clean_data.unique())
            if len(sorted_vals) <= 20:  # M√°ximo 20 valores √∫nicos para ratings
                return 'ordinal'
    
    # Muchos valores, probablemente decimales -> Continua
    return 'continua'

def graficar_numericas(df, variables_numericas, auto_detect=True, 
                      tipos_especificos=None, figsize=(16, 12)):
    """
    Funci√≥n para graficar variables num√©ricas seg√∫n su tipo espec√≠fico.
    
    Esta funci√≥n es la versi√≥n mejorada que:
    - Detecta autom√°ticamente el tipo de variable (ordinal, discreta, continua)
    - Aplica visualizaciones espec√≠ficas seg√∫n el tipo
    - Maneja outliers inteligentemente para variables continuas
    - Muestra histograma y boxplot lado a lado (sin superposici√≥n)
    - Usa colores profesionales y estad√≠sticas en la derecha
    
    Args:
        df: DataFrame con los datos
        variables_numericas: Lista de nombres de columnas num√©ricas
        auto_detect: Si True, detecta autom√°ticamente el tipo de variable
        tipos_especificos: Dict con tipos espec√≠ficos {'variable': 'tipo'}
        figsize: Tama√±o de la figura
        
    Returns:
        None (muestra los gr√°ficos)
    """
    
    # Detectar tipos si no se especifican
    if tipos_especificos is None:
        tipos_especificos = {}
    
    tipos_detectados = {}
    for var in variables_numericas:
        if var in df.columns:
            if var in tipos_especificos:
                tipos_detectados[var] = tipos_especificos[var]
            elif auto_detect:
                tipos_detectados[var] = detectar_tipo_numerico(df[var], var)
            else:
                tipos_detectados[var] = 'continua'  # Por defecto
    
    # Mostrar tipos detectados
    print("üîç TIPOS DE VARIABLES DETECTADOS:")
    print("="*60)
    for var, tipo in tipos_detectados.items():
        print(f"üìä {var}: {tipo.upper()}")
    print()
    
    # Calcular layout
    n_vars = len(variables_numericas)
    n_cols = 3
    n_rows = (n_vars + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    
    # Asegurar que axes sea array 2D
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    elif n_cols == 1:
        axes = axes.reshape(-1, 1)
    
    # Paleta de colores mejorada y coherente
    colores = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#3A1772', 
               '#6B5B95', '#88B04B', '#F7CAC9', '#92A8D1', '#955251']
    
    # Bucle for principal para cada variable
    for i, variable in enumerate(variables_numericas):
        row = i // n_cols
        col = i % n_cols
        ax = axes[row, col]
        
        if variable not in df.columns:
            ax.text(0.5, 0.5, f'Variable "{variable}"\nno encontrada', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{variable} (No encontrada)')
            continue
        
        data = df[variable].dropna()
        if len(data) == 0:
            ax.text(0.5, 0.5, f'Variable "{variable}"\nsin datos v√°lidos', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{variable} (Sin datos)')
            continue
        
        # Convertir a num√©rico
        try:
            data = pd.to_numeric(data)
        except:
            ax.text(0.5, 0.5, f'Variable "{variable}"\nno es num√©rica', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{variable} (No num√©rica)')
            continue
        
        tipo_var = tipos_detectados.get(variable, 'continua')
        color = colores[i % len(colores)]
        
        # Graficar seg√∫n el tipo de variable
        if tipo_var == 'ordinal':
            # VARIABLES ORDINALES: Gr√°fico de barras ordenado
            value_counts = data.value_counts().sort_index()  # Ordenar por valor
            
            bars = ax.bar(value_counts.index, value_counts.values, 
                         color=color, alpha=0.8, edgecolor='white', linewidth=1)
            
            # A√±adir valores en las barras
            for bar, value in zip(bars, value_counts.values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                       f'{value}', ha='center', va='bottom', fontsize=9)
            
            ax.set_title(f'ORDINAL - {variable}', fontweight='bold', color='#8B5A96', fontsize=11)
            ax.set_xlabel(f'{variable} (valores ordenados)')
            ax.set_ylabel('Frecuencia')
            ax.grid(axis='y', alpha=0.3)
            
            # Estad√≠sticas relevantes para ordinales
            moda = data.mode().iloc[0] if len(data.mode()) > 0 else 'N/A'
            mediana = data.median()
            ax.text(0.98, 0.98, f'Moda: {moda}\nMediana: {mediana}', 
                   transform=ax.transAxes, va='top', ha='right', fontsize=8,
                   bbox=dict(boxstyle='round', facecolor='#E6F3FF', alpha=0.9, 
                            edgecolor='#8B5A96', linewidth=0.5))
        
        elif tipo_var == 'discreta':
            # VARIABLES DISCRETAS: Gr√°fico de barras + estad√≠sticas
            value_counts = data.value_counts().sort_index()
            
            # Limitar a los 20 valores m√°s frecuentes si hay muchos
            if len(value_counts) > 20:
                value_counts = data.value_counts().head(20)  # Los m√°s frecuentes
                titulo_extra = f" (Top 20 de {data.nunique()} valores)"
            else:
                titulo_extra = f" ({data.nunique()} valores √∫nicos)"
            
            bars = ax.bar(range(len(value_counts)), value_counts.values,
                         color=color, alpha=0.8, edgecolor='white', linewidth=1)
            
            ax.set_xticks(range(len(value_counts)))
            ax.set_xticklabels([str(x) for x in value_counts.index], rotation=45)
            
            # A√±adir valores en barras principales
            for bar, value in zip(bars, value_counts.values):
                if value > max(value_counts.values) * 0.05:  # Solo mostrar valores significativos
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value}', ha='center', va='bottom', fontsize=8)
            
            ax.set_title(f'DISCRETA - {variable}{titulo_extra}', fontweight='bold', color='#2E8B57', fontsize=11)
            ax.set_xlabel(f'{variable}')
            ax.set_ylabel('Frecuencia')
            ax.grid(axis='y', alpha=0.3)
            
            # Estad√≠sticas para discretas
            media = data.mean()
            std = data.std()
            ax.text(0.98, 0.98, f'Media: {media:.2f}\nStd: {std:.2f}\nM√≠n: {data.min()}\nM√°x: {data.max()}', 
                   transform=ax.transAxes, va='top', ha='right', fontsize=8,
                   bbox=dict(boxstyle='round', facecolor='#E8F5E8', alpha=0.9, 
                            edgecolor='#2E8B57', linewidth=0.5))
        
        elif tipo_var == 'continua':
            # VARIABLES CONTINUAS: Histograma y boxplot LADO A LADO
            
            # Detectar si hay outliers o cola larga
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # Determinar si hay outliers significativos
            outliers = data[(data < lower_bound) | (data > upper_bound)]
            outlier_percentage = len(outliers) / len(data) * 100
            
            # Calcular percentiles para determinar el rango de visualizaci√≥n
            p95 = data.quantile(0.95)
            
            # Si hay muchos outliers (>5%), usar percentil 95 como l√≠mite superior
            if outlier_percentage > 5:
                max_visible = p95
                titulo_extra = " (sin outliers extremos)"
            else:
                max_visible = data.max()
                titulo_extra = ""
            
            # Filtrar datos para el histograma principal
            data_filtered = data[data <= max_visible]
            
            # Dividir el espacio: 75% histograma, 25% boxplot
            pos = ax.get_position()
            
            # √Årea del histograma (75% del ancho)
            hist_width = pos.width * 0.75
            ax_hist = fig.add_axes([pos.x0, pos.y0, hist_width, pos.height])
            
            # √Årea del boxplot (25% del ancho, al lado derecho)
            box_width = pos.width * 0.25
            ax_box = fig.add_axes([pos.x0 + hist_width, pos.y0, box_width, pos.height])
            
            # Ocultar el eje original
            ax.set_visible(False)
            
            # HISTOGRAMA (lado izquierdo)
            n_bins = min(30, int(np.sqrt(len(data_filtered))))
            n, bins, patches = ax_hist.hist(data_filtered, bins=n_bins, density=True, alpha=0.7,
                                          color=color, edgecolor='white', linewidth=1, orientation='vertical')
            
            # Curva de densidad
            try:
                from scipy import stats
                density = stats.gaussian_kde(data_filtered)
                xs = np.linspace(data_filtered.min(), data_filtered.max(), 200)
                ax_hist.plot(xs, density(xs), color='#DC2626', linewidth=2.5, 
                           label='Densidad', alpha=0.8)
            except:
                pass
            
            # L√≠neas estad√≠sticas en el histograma
            media = data.mean()
            mediana = data.median()
            
            ax_hist.axvline(media, color='#DC2626', linestyle='--', alpha=0.8, 
                          linewidth=2, label=f'Media: {media:.2f}')
            ax_hist.axvline(mediana, color='#2563EB', linestyle='--', alpha=0.8, 
                          linewidth=2, label=f'Mediana: {mediana:.2f}')
            
            # BOXPLOT (lado derecho, vertical)
            bp = ax_box.boxplot(data_filtered, vert=True, patch_artist=True, 
                              showfliers=True, widths=0.6)
            
            # Colorear el boxplot
            bp['boxes'][0].set_facecolor(color)
            bp['boxes'][0].set_alpha(0.8)
            bp['medians'][0].set_color('#1E3A8A')
            bp['medians'][0].set_linewidth(2)
            
            # Configurar histograma
            ax_hist.set_title(f'CONTINUA - {variable}{titulo_extra}', 
                            fontweight='bold', color='#1E3A8A', fontsize=11)
            ax_hist.set_xlabel(f'{variable}')
            ax_hist.set_ylabel('Densidad')
            ax_hist.legend(fontsize=8, loc='upper center')
            ax_hist.grid(alpha=0.3)
            
            # Configurar boxplot
            ax_box.set_title('Boxplot', fontsize=9, color='#1E3A8A')
            ax_box.set_ylabel('')
            ax_box.set_xlabel('')
            ax_box.set_xticks([])
            ax_box.grid(alpha=0.3, axis='y')
            
            # Asegurar que ambos ejes tengan el mismo rango Y
            hist_ylim = ax_hist.get_ylim()
            data_min, data_max = data_filtered.min(), data_filtered.max()
            ax_box.set_ylim([data_min, data_max])
            
            # Estad√≠sticas detalladas
            std = data.std()
            skewness = data.skew()
            
            if outlier_percentage > 5:
                stats_text = f'Media: {media:.3f}\nStd: {std:.3f}\nAsimetr√≠a: {skewness:.3f}\nOutliers: {outlier_percentage:.1f}%'
            else:
                stats_text = f'Media: {media:.3f}\nStd: {std:.3f}\nAsimetr√≠a: {skewness:.3f}'
            
            ax_hist.text(0.02, 0.98, stats_text, 
                       transform=ax_hist.transAxes, va='top', ha='left', fontsize=8,
                       bbox=dict(boxstyle='round', facecolor='#F0F8FF', alpha=0.9, 
                                edgecolor='#1E3A8A', linewidth=0.5))
        
        # Para variables no continuas, mejorar aspecto
        if tipo_var != 'continua':
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['left'].set_linewidth(0.5)
            ax.spines['bottom'].set_linewidth(0.5)
    
    # Ocultar subplots vac√≠os (solo para no-continuas)
    for i in range(n_vars, n_rows * n_cols):
        row = i // n_cols
        col = i % n_cols
        if axes[row, col].get_visible():  # Solo si no es una variable continua
            axes[row, col].set_visible(False)
    
    # Mejorar el estilo general
    plt.style.use('default')
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3
    
    #plt.tight_layout(pad=3.0)
    plt.show()

def analisis_tipos_numericos(df, variables_numericas):
    """
    Funci√≥n para analizar en detalle los tipos de variables num√©ricas
    """
    print("üî¨ AN√ÅLISIS DETALLADO DE TIPOS DE VARIABLES NUM√âRICAS")
    print("="*80)
    
    for variable in variables_numericas:
        if variable not in df.columns:
            print(f"\n‚ùå {variable}: No encontrada en el DataFrame")
            continue
        
        data = df[variable].dropna()
        if len(data) == 0:
            print(f"\n‚ùå {variable}: Sin datos v√°lidos")
            continue
        
        try:
            data = pd.to_numeric(data)
        except:
            print(f"\n‚ùå {variable}: No es num√©rica")
            continue
        
        tipo_detectado = detectar_tipo_numerico(df[variable], variable)
        
        print(f"\nüìä Variable: {variable}")
        print(f"üîç Tipo detectado: {tipo_detectado.upper()}")
        print("-" * 50)
        
        print(f"Total valores: {len(data):,}")
        print(f"Valores √∫nicos: {data.nunique():,}")
        print(f"% valores √∫nicos: {(data.nunique()/len(data)*100):.2f}%")
        print(f"Rango: {data.min()} - {data.max()}")
        print(f"¬øSon enteros?: {'S√≠' if all(data == data.astype(int)) else 'No'}")
        
        # Mostrar valores m√°s frecuentes
        top_values = data.value_counts().head(5)
        print(f"Top 5 valores m√°s frecuentes:")
        for val, count in top_values.items():
            print(f"  {val}: {count} veces ({count/len(data)*100:.1f}%)")
        
        # Recomendaciones seg√∫n el tipo
        if tipo_detectado == 'ordinal':
            print("üí° Recomendaci√≥n: Usar gr√°ficos de barras ordenados, calcular moda y mediana")
        elif tipo_detectado == 'discreta':
            print("üí° Recomendaci√≥n: Usar gr√°ficos de barras, analizar distribuci√≥n de frecuencias")
        elif tipo_detectado == 'continua':
            print("üí° Recomendaci√≥n: Usar histogramas, boxplots, analizar distribuci√≥n y outliers")


# =============================================================================
# AN√ÅLISIS BIVARIADO - RELACI√ìN FEATURE ‚Üî TARGET
# =============================================================================

def graficar_variable_con_target(
    df: pd.DataFrame, feature: str, target: str,
    problema: str = "auto", max_categories: int = 20, show: bool = True,
):
    """
    Grafica la relaci√≥n entre una feature y el target.
    
    Esta funci√≥n detecta autom√°ticamente el tipo de problema y elige la visualizaci√≥n
    m√°s apropiada:
    
    CLASIFICACI√ìN:
    - Feature num√©rica ‚Üí Boxplot horizontal
    - Feature categ√≥rica ‚Üí Gr√°fico de barras apiladas
    
    REGRESI√ìN:
    - Feature num√©rica ‚Üí Scatter plot
    - Feature categ√≥rica ‚Üí Boxplot vertical
    
    Args:
        df: DataFrame con los datos
        feature: Nombre de la columna feature
        target: Nombre de la columna target
        problema: 'clasificacion', 'regresion', o 'auto' (detecta autom√°ticamente)
        max_categories: M√°ximo n√∫mero de categor√≠as a mostrar
        show: Si True, muestra el gr√°fico
        
    Returns:
        tuple: (fig, ax) para manipulaci√≥n posterior
    """
    if feature not in df.columns or target not in df.columns:
        raise ValueError("feature o target no est√°n en el DataFrame.")
    s_feat, s_tgt = df[feature], df[target]

    def _is_categorical(s):
        return s.dtype.name in ("object", "category") or s.nunique(dropna=True) <= 15

    if problema == "auto":
        problema = "clasificacion" if _is_categorical(s_tgt) else "regresion"

    fig, ax = plt.subplots(figsize=(8, 4.2))

    if problema == "clasificacion":
        if is_numeric_dtype(s_feat):
            sns.boxplot(data=df, x=feature, y=target, whis=1.5, ax=ax, orient="h")
            ax.set_title(f"{feature} por {target}"); ax.grid(alpha=0.3, axis="x")
        else:
            ct = (
                df[[feature, target]].dropna().value_counts(normalize=True)
                  .rename("prop").reset_index()
            )
            if max_categories and df[feature].nunique(dropna=True) > max_categories:
                top = df[feature].value_counts().index[:max_categories]
                ct = ct[ct[feature].isin(top)]
            sns.barplot(data=ct, x="prop", y=feature, hue=target, ax=ax, orient="h")
            ax.set_title(f"Proporciones de {feature} por {target}")
            ax.grid(alpha=0.3, axis="x"); ax.legend(title=target, bbox_to_anchor=(1.02,1), loc="upper left")
    else:
        if is_numeric_dtype(s_feat):
            ax.scatter(s_feat, s_tgt, alpha=0.6, edgecolors="none")
            ax.set_xlabel(feature); ax.set_ylabel(target)
            ax.set_title(f"{feature} vs {target}"); ax.grid(alpha=0.3)
        else:
            s = df.copy()
            if max_categories and s[feature].nunique(dropna=True) > max_categories:
                top = s[feature].value_counts().index[:max_categories]
                s = s[s[feature].isin(top)]
            sns.boxplot(data=s, x=target, y=feature, whis=1.5, ax=ax, orient="h")
            ax.set_title(f"{target} por {feature}"); ax.grid(alpha=0.3, axis="x")

    if show: plt.show()
    return fig, ax

# =============================================================================
# AN√ÅLISIS DE CORRELACIONES, ASOCIACIONES, VIF Y MUTUAL INFORMATION
# =============================================================================

def matriz_correlacion_numericas(df: pd.DataFrame, cols: Optional[Iterable[str]]=None, metodo: str="spearman") -> pd.DataFrame:
    """
    Calcula la matriz de correlaci√≥n para variables num√©ricas.
    
    Args:
        df: DataFrame con los datos
        cols: Columnas espec√≠ficas a analizar (si None, usa todas las num√©ricas)
        metodo: M√©todo de correlaci√≥n ('pearson', 'spearman', 'kendall')
        
    Returns:
        DataFrame con la matriz de correlaci√≥n
    """
    cols = _ensure_list(cols) or df.select_dtypes(include="number").columns.tolist()
    return df[cols].corr(method=metodo)

def _cramers_v_corrected(confusion: pd.DataFrame) -> float:
    """
    Calcula el coeficiente V de Cram√©r corregido para medir asociaci√≥n entre categ√≥ricas.
    
    Esta es una versi√≥n corregida que maneja mejor casos extremos y bias.
    
    Args:
        confusion: Matriz de contingencia (crosstab)
        
    Returns:
        float: Valor del coeficiente V de Cram√©r (0-1)
    """
    chi2 = stats.chi2_contingency(confusion.values)[0]
    n = confusion.sum().sum(); r, k = confusion.shape
    phi2 = chi2 / n
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
    rcorr = r - ((r-1)**2)/(n-1); kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / max(1e-12, min((kcorr-1), (rcorr-1))))

def matriz_asociacion_categoricas(df: pd.DataFrame, cols: Optional[Iterable[str]]=None) -> pd.DataFrame:
    """
    Calcula la matriz de asociaci√≥n para variables categ√≥ricas usando V de Cram√©r.
    
    Args:
        df: DataFrame con los datos
        cols: Columnas espec√≠ficas a analizar (si None, usa todas las categ√≥ricas)
        
    Returns:
        DataFrame con la matriz de asociaci√≥n (valores 0-1)
    """
    cols = _ensure_list(cols) or df.select_dtypes(include=["object","category","bool"]).columns.tolist()
    cols = [c for c in cols if df[c].nunique(dropna=True) > 1]  # Excluir constantes
    mat = pd.DataFrame(index=cols, columns=cols, dtype=float)
    
    # Calcular asociaciones por pares
    for i, c1 in enumerate(cols):
        for j, c2 in enumerate(cols):
            if i > j: continue  # Evitar c√°lculos duplicados
            if i == j: mat.loc[c1, c2] = 1.0  # Diagonal = 1
            else:
                confusion = pd.crosstab(df[c1], df[c2])
                mat.loc[c1, c2] = mat.loc[c2, c1] = _cramers_v_corrected(confusion)
    return mat

def calcular_vif(df: pd.DataFrame, cols: Optional[Iterable[str]]=None) -> pd.DataFrame:
    """
    Calcula el Variance Inflation Factor (VIF) para detectar multicolinealidad.
    
    El VIF mide cu√°nto se infla la varianza de un coeficiente debido a la correlaci√≥n
    con otras variables. Valores > 5-10 indican problemas de multicolinealidad.
    
    Args:
        df: DataFrame con los datos
        cols: Columnas espec√≠ficas a analizar (si None, usa todas las num√©ricas)
        
    Returns:
        DataFrame con VIF por variable, ordenado descendente
        
    Raises:
        ImportError: Si statsmodels no est√° disponible
        ValueError: Si no hay suficientes columnas num√©ricas
    """
    cols = _ensure_list(cols) or df.select_dtypes(include="number").columns.tolist()
    X = df[cols].replace([np.inf, -np.inf], np.nan).dropna()
    if X.empty or len(X.columns) < 2:
        raise ValueError("Se necesitan >= 2 columnas num√©ricas sin NaN para VIF.")
    
    # Estandarizar variables
    X = (X - X.mean()) / X.std(ddof=0)
    
    if not _HAS_STATSMODELS:
        raise ImportError("statsmodels no est√° disponible para calcular VIF.")
    
    # Import local por seguridad
    from statsmodels.stats.outliers_influence import variance_inflation_factor as _vif
    vif_values = [{"variable": c, "VIF": float(_vif(X.values, i))} for i, c in enumerate(X.columns)]
    return pd.DataFrame(vif_values).sort_values("VIF", ascending=False).reset_index(drop=True)

def informacion_mutua_con_target(df: pd.DataFrame, features: Iterable[str], target: str, problema: str="auto") -> pd.DataFrame:
    """
    Calcula la Mutual Information (MI) entre features y target.
    
    La MI mide la dependencia estad√≠stica entre variables, capturando relaciones
    tanto lineales como no lineales. Es muy √∫til para feature selection.
    
    Args:
        df: DataFrame con los datos
        features: Lista de features a analizar
        target: Variable objetivo
        problema: 'clasificacion', 'regresion', o 'auto' (detecta autom√°ticamente)
        
    Returns:
        DataFrame con MI por feature, ordenado descendente
        
    Raises:
        ImportError: Si scikit-learn no est√° disponible
    """
    if not _HAS_SKLEARN:
        raise ImportError("scikit-learn no disponible: instala 'scikit-learn'.")
    
    # Filtrar features v√°lidas
    features = [f for f in _ensure_list(features) if f in df.columns and f != target]
    
    # Preparar datos: one-hot encoding para categ√≥ricas
    X = pd.get_dummies(df[features].copy(), drop_first=False)
    y = df[target].copy()
    
    # Limpieza b√°sica de datos
    X = X.replace([np.inf, -np.inf], np.nan).fillna(method="ffill").fillna(method="bfill")
    y = y.replace([np.inf, -np.inf], np.nan).dropna()
    
    # Alinear √≠ndices
    common_idx = X.index.intersection(y.index)
    X = X.loc[common_idx]
    y = y.loc[common_idx]
    
    # Detectar tipo de problema
    if problema == "auto":
        problema = "clasificacion" if y.dtype.name in ("object","category") or y.nunique(dropna=True) <= 15 else "regresion"
    
    # Calcular MI seg√∫n el tipo de problema
    if problema == "clasificacion":
        if y.dtype.name in ("object","category"): 
            y, _ = pd.factorize(y)  # Convertir categ√≥ricas a num√©ricas
        scores = mutual_info_classif(X, y, discrete_features='auto', random_state=0)
    else:
        scores = mutual_info_regression(X, y, discrete_features='auto', random_state=0)
    
    return pd.DataFrame({"feature": X.columns, "mi": scores}).sort_values("mi", ascending=False).reset_index(drop=True)

def heatmap_matriz(matriz: pd.DataFrame, titulo: Optional[str]=None, cmap: str="viridis", annot: bool=False, fmt: str=".2f", show: bool=True):
    """
    Crea un heatmap de una matriz (correlaci√≥n, asociaci√≥n, etc.).
    
    Args:
        matriz: DataFrame con la matriz a visualizar
        titulo: T√≠tulo del gr√°fico
        cmap: Mapa de colores (ej: 'viridis', 'RdBu_r', 'Blues')
        annot: Si True, muestra los valores en las celdas
        fmt: Formato de los valores (ej: '.2f', '.3f')
        show: Si True, muestra el gr√°fico
        
    Returns:
        tuple: (fig, ax) para manipulaci√≥n posterior
    """
    # Tama√±o din√°mico basado en el n√∫mero de columnas/filas
    fig, ax = plt.subplots(figsize=(max(6, 0.6*len(matriz.columns)), max(5, 0.6*len(matriz.index))))
    sns.heatmap(matriz, cmap=cmap, annot=annot, fmt=fmt, square=False, cbar=True, ax=ax)
    if titulo: ax.set_title(titulo, fontweight="bold")
    plt.tight_layout()
    if show: plt.show()
    return fig, ax


__all__ = [
    "detectar_tipos_columnas",
    "resumen_nulos", "resumen_numericas", "reporte_categorias_raras",
    "graficar_categoricas", "graficar_numericas", "graficar_variables_numericas_completo",
    "graficar_variable_con_target",
    "matriz_correlacion_numericas", "matriz_asociacion_categoricas",
    "calcular_vif", "informacion_mutua_con_target",
    "heatmap_matriz",
]
